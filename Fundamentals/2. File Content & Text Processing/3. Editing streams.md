***

Editing streams: `sed` `awk` `tr` `cut` `paste` `split` `join`  

***
***
***
# `sed` Command in Linux

`sed` (**S**tream **Ed**itor) is a powerful non-interactive text-processing tool in Linux/Unix. It reads input line by line, applies commands, and outputs the result. It is commonly used for **searching, finding, replacing, inserting, deleting, and transforming text** in files or streams.

* * *

## 1\. What is `sed`

`sed` is a **stream editor** that:

- Processes text line-by-line
    
- Does not modify files unless explicitly told
    
- Is extremely fast and scriptable
    

Typical use cases:

- Replacing text in files
    
- Deleting specific lines
    
- Editing configuration files
    
- Text transformation in pipelines
    

* * *

## 2\. How `sed` Works

1.  Reads one line into **pattern space**
    
2.  Applies commands
    
3.  Prints the result (by default)
    
4.  Moves to the next line
    

```
Input → Pattern Space → Commands → Output
```

* * *

## 3\. Basic Syntax

```bash
sed [OPTIONS] 'COMMAND' file
```

Or using stdin:

```bash
echo "hello world" | sed 's/world/Linux/'
```

* * *

## 4\. Common Options

| Option | Description |
| --- | --- |
| `-n` | Suppress automatic printing |
| `-i` | Edit file in-place |
| `-e` | Multiple commands |
| `-f` | Read commands from file |
| `-r` / `-E` | Extended regex |

* * *

## 5\. Printing Lines

### Print all lines (default)

```bash
sed '' file.txt
```

### Print specific lines

```bash
sed -n '2p' file.txt
```

### Print a range

```bash
sed -n '2,5p' file.txt
```

* * *

## 6\. Searching (Pattern Matching)

```bash
sed -n '/error/p' logfile.txt
```

Prints lines containing `error`

* * *

## 7\. Substitution (Find & Replace)

### Basic replacement

```bash
sed 's/old/new/' file.txt
```

### Global replacement

```bash
sed 's/old/new/g' file.txt
```

### Case-insensitive

```bash
sed 's/linux/Linux/i' file.txt
```

### Replace nth occurrence

```bash
sed 's/foo/bar/2' file.txt
```

* * *

## 8\. Deleting Lines

### Delete a specific line

```bash
sed '3d' file.txt
```

### Delete a range

```bash
sed '3,6d' file.txt
```

### Delete matching pattern

```bash
sed '/debug/d' file.txt
```

* * *

## 9\. Inserting and Appending Text

### Insert before a line

```bash
sed '2i This is inserted' file.txt
```

### Append after a line

```bash
sed '2a This is appended' file.txt
```

### Insert at beginning

```bash
sed '1i HEADER' file.txt
```

* * *

## 10\. Replacing Entire Lines

```bash
sed '3c This line is replaced' file.txt
```

Replace lines matching pattern:

```bash
sed '/error/c ERROR FOUND' file.txt
```

* * *

## 11\. Using Line Numbers

```bash
sed -n '1,10p' file.txt
```

Last line:

```bash
sed '$p' file.txt
```

* * *

## 12\. Regular Expressions in `sed`

### Basic regex

| Symbol | Meaning |
| --- | --- |
| `.` | Any character |
| `*` | Zero or more |
| `^` | Start of line |
| `$` | End of line |

### Example

```bash
sed '/^#/d' config.conf
```

Deletes comments

* * *

## 13\. Multiple Commands

```bash
sed -e 's/foo/bar/' -e 's/hello/hi/' file.txt
```

Using semicolon:

```bash
sed 's/foo/bar/; s/hello/hi/' file.txt
```

* * *

## 14\. Using `sed` with Files

Read file:

```bash
sed 's/yes/no/' input.txt
```

Write to new file:

```bash
sed 's/yes/no/' input.txt > output.txt
```

* * *

## 15\. In-place Editing (`-i`)

```bash
sed -i 's/8080/9090/' server.conf
```

Backup before editing:

```bash
sed -i.bak 's/8080/9090/' server.conf
```

* * *

## 16\. Working with Multiple Files

```bash
sed 's/error/warning/' file1.txt file2.txt
```

* * *

## 17\. Using Address Ranges

```bash
sed '/START/,/END/d' file.txt
```

Line numbers + pattern:

```bash
sed '1,/ERROR/d' file.txt
```

* * *

## 18\. Backreferences & Groups

```bash
sed 's/\(foo\)bar/\1baz/' file.txt
```

Extended regex:

```bash
sed -E 's/(foo)bar/\1baz/' file.txt
```

* * *

## 19\. Hold Space & Pattern Space

```bash
sed '1h; 1!H; $!d; x; s/\n/, /g' file.txt
```

Used for advanced multi-line processing

* * *

## 20\. GNU vs BSD `sed`

| Feature | GNU | BSD (macOS) |
| --- | --- | --- |
| `-i` | Works directly | Requires backup extension |
| `-r` | Extended regex | Use `-E` |

* * *

## 21\. Practical Real-World Examples

### Remove empty lines

```bash
sed '/^$/d' file.txt
```

### Trim whitespace

```bash
sed 's/^ *//; s/ *$//' file.txt
```

### Comment out lines

```bash
sed 's/^/#/' file.txt
```

### Replace tabs with spaces

```bash
sed 's/\t/  /g' file.txt
```

* * *

## 22\. Tips & Best Practices

- Always test without `-i`
    
- Use backups when editing files
    
- Prefer single quotes `'`
    
- Combine with `grep`, `awk`, `cut`
    

* * *

## 23\. Summary Cheat Sheet

```bash
sed 's/a/b/' file        # replace first match
sed 's/a/b/g' file      # replace all
sed '3d' file           # delete line 3
sed -n '5p' file        # print line 5
sed '/pat/d' file       # delete pattern
sed -i 's/x/y/' file    # in-place edit
```

* * *

## Final Notes

`sed` is extremely powerful for automation, scripting, and system administration. Mastering it will greatly improve your Linux text-processing skills.

* * *

* * *

# `awk` Command in Linux

`awk` is a powerful **pattern scanning and text processing language** used in Linux/Unix. It is especially suited for working with **structured text**, such as columns in files, logs, CSVs, and command output.

* * *

## 1\. What is `awk`

`awk` is a text-processing tool and programming language that:

- Reads input line by line (records)
    
- Splits each line into fields
    
- Executes actions based on patterns
    

Typical use cases:

- Extracting columns from text
    
- Generating reports
    
- Processing log files
    
- Data transformation
    

* * *

## 2\. How `awk` Works

```
Input → Record → Fields → Pattern Match → Action → Output
```

Each line is a **record**. Each word/column is a **field**.

* * *

## 3\. Basic Syntax

```bash
awk 'pattern { action }' file
```

Example:

```bash
awk '{ print $0 }' file.txt
```

* * *

## 4\. Fields and Records

| Element | Description |
| --- | --- |
| `$0` | Entire line |
| `$1` | First field |
| `$2` | Second field |
| `NF` | Number of fields |
| `NR` | Record (line) number |

Example:

```bash
awk '{ print $1, $3 }' file.txt
```

* * *

## 5\. Built-in Variables

| Variable | Meaning |
| --- | --- |
| `NR` | Line number |
| `NF` | Number of fields |
| `FS` | Field separator |
| `OFS` | Output field separator |
| `RS` | Record separator |
| `ORS` | Output record separator |

* * *

## 6\. Printing Data

Print specific fields:

```bash
awk '{ print $1 }' file.txt
```

Print with formatting:

```bash
awk '{ printf "%s %d\n", $1, $2 }' file.txt
```

* * *

## 7\. Pattern Matching

Print lines matching a pattern:

```bash
awk '/error/ { print }' logfile.txt
```

Print lines where field equals value:

```bash
awk '$3 == "root" { print }' /etc/passwd
```

* * *

## 8\. Conditional Logic

```bash
awk '{ if ($2 > 50) print $1 }' scores.txt
```

Using `else`:

```bash
awk '{ if ($2 >= 40) print "PASS"; else print "FAIL" }' scores.txt
```

* * *

## 9\. Arithmetic and String Operations

```bash
awk '{ sum += $2 } END { print sum }' data.txt
```

String length:

```bash
awk '{ print length($1) }' file.txt
```

* * *

## 10\. Using Regular Expressions

```bash
awk '$1 ~ /^A/ { print }' names.txt
```

Negation:

```bash
awk '$1 !~ /test/ { print }' file.txt
```

* * *

## 11\. Field Separators

Default separator is space.

Custom separator:

```bash
awk -F ':' '{ print $1 }' /etc/passwd
```

Multiple separators:

```bash
awk -F '[,:]' '{ print $1 }' file.txt
```

* * *

## 12\. Control Flow (if, while, for)

For loop:

```bash
awk '{ for (i=1; i<=NF; i++) print $i }' file.txt
```

While loop:

```bash
awk '{ i=1; while (i<=NF) print $i; i++ }' file.txt
```

* * *

## 13\. Built-in Functions

| Function | Purpose |
| --- | --- |
| `length()` | String length |
| `substr()` | Substring |
| `index()` | Position |
| `tolower()` | Lowercase |
| `toupper()` | Uppercase |

Example:

```bash
awk '{ print toupper($1) }' file.txt
```

* * *

## 14\. User-Defined Functions

```bash
awk '
function square(x) { return x*x }
{ print square($2) }
' file.txt
```

* * *

## 15\. Working with Files

Multiple files:

```bash
awk '{ print FNR, $0 }' file1 file2
```

Redirect output:

```bash
awk '{ print $1 }' file.txt > out.txt
```

* * *

## 16\. Processing CSV and Logs

CSV:

```bash
awk -F ',' '{ print $1, $3 }' data.csv
```

Apache logs:

```bash
awk '{ print $1, $7 }' access.log
```

* * *

## 17\. Associative Arrays

```bash
awk '{ count[$1]++ } END { for (k in count) print k, count[k] }' file.txt
```

* * *

## 18\. Multiple Commands and Scripts

Inline:

```bash
awk '{ sum += $2 } END { print sum }' file.txt
```

Script file:

```bash
awk -f script.awk file.txt
```

* * *

## 19\. GNU `awk` vs POSIX `awk`

| Feature | GNU awk | POSIX awk |
| --- | --- | --- |
| Arrays | Yes | Yes |
| Regex | Extended | Basic |
| `gensub()` | Yes | No  |

* * *

## 20\. Practical Real-World Examples

Remove duplicates:

```bash
awk '!seen[$0]++' file.txt
```

Sum column:

```bash
awk '{ total += $3 } END { print total }' sales.txt
```

Print last column:

```bash
awk '{ print $NF }' file.txt
```

* * *

## 21\. Tips and Best Practices

- Use `awk` for column-based data
    
- Combine with `sed` and `grep`
    
- Prefer scripts for complex logic
    
- Use `printf` for formatting
    

* * *

## 22\. Summary Cheat Sheet

```bash
awk '{ print $1 }' file
awk -F ':' '{ print $1 }' file
awk '$3 > 50' file
awk '{ sum += $2 } END { print sum }'
awk '!seen[$0]++' file
```

* * *

## Final Notes

`awk` is ideal when working with structured text and reports. It complements `sed` (line editing) and `grep` (searching) and is a core skill for Linux users.

* * *

* * *

# `tr` Command in Linux

`tr` (**tr**anslate) is a simple yet powerful Linux command used to **translate, squeeze, or delete characters** from standard input. Unlike `sed` and `awk`, `tr` works **only on characters**, not on lines or patterns.

* * *

## 1\. What is `tr`

`tr` reads from **standard input** and writes to **standard output**. It is mainly used for:

- Character replacement
    
- Case conversion
    
- Removing unwanted characters
    
- Normalizing text
    

Typical use cases:

- Convert lowercase to uppercase
    
- Remove digits or special characters
    
- Compress repeated spaces
    

* * *

## 2\. How `tr` Works

```
stdin → character translation/deletion → stdout
```

Important points:

- `tr` cannot read files directly
    
- Input must come via pipe or redirection
    
- It works one character at a time
    

* * *

## 3\. Basic Syntax

```bash
tr [OPTIONS] SET1 [SET2]
```

Example:

```bash
echo "hello" | tr 'a-z' 'A-Z'
```

* * *

## 4\. Character Sets

| Set | Description |
| --- | --- |
| `abc` | Explicit characters |
| `a-z` | Range |
| `0-9` | Digits |
| `\n` | Newline |

Example:

```bash
echo "123abc" | tr '0-9' 'x'
```

* * *

## 5\. Translating Characters

Replace characters from SET1 with SET2:

```bash
echo "banana" | tr 'a' 'o'
```

Multiple replacements:

```bash
echo "abc123" | tr 'a-c' 'x-z'
```

* * *

## 6\. Case Conversion

Lowercase to uppercase:

```bash
echo "linux" | tr 'a-z' 'A-Z'
```

Uppercase to lowercase:

```bash
echo "LINUX" | tr 'A-Z' 'a-z'
```

Using character classes:

```bash
echo "Linux" | tr '[:lower:]' '[:upper:]'
```

* * *

## 7\. Deleting Characters

Delete characters using `-d`:

```bash
echo "abc123" | tr -d '0-9'
```

Delete whitespace:

```bash
echo "a b  c" | tr -d ' '
```

* * *

## 8\. Squeezing Repeated Characters

Use `-s` to squeeze repeats:

```bash
echo "a   b    c" | tr -s ' '
```

Squeeze newlines:

```bash
tr -s '\n' < file.txt
```

* * *

## 9\. Complement Sets

Use `-c` to match characters **not in SET**:

```bash
echo "abc123" | tr -cd '0-9'
```

Keep only letters:

```bash
echo "abc123" | tr -cd 'a-zA-Z'
```

* * *

## 10\. Character Classes

| Class | Meaning |
| --- | --- |
| `[:lower:]` | Lowercase letters |
| `[:upper:]` | Uppercase letters |
| `[:digit:]` | Digits |
| `[:space:]` | Whitespace |
| `[:alpha:]` | Letters |
| `[:alnum:]` | Letters and digits |

Example:

```bash
echo "Hello123" | tr -d '[:digit:]'
```

* * *

## 11\. Working with Files

Since `tr` does not take filenames:

```bash
tr 'a-z' 'A-Z' < input.txt > output.txt
```

* * *

## 12\. Common Pipelines

With `cat`:

```bash
cat file.txt | tr 'a-z' 'A-Z'
```

With `sed`:

```bash
sed 's/,/ /g' file.txt | tr -s ' '
```

With `awk`:

```bash
awk '{ print $1 }' file.txt | tr 'a-z' 'A-Z'
```

* * *

## 13\. Limitations of `tr`

- No regex support
    
- Cannot work line-by-line
    
- Cannot edit files in place
    
- Only character-level operations
    

Use `sed` or `awk` for complex logic.

* * *

## 14\. Practical Real-World Examples

Remove CRLF (Windows line endings):

```bash
tr -d '\r' < file.txt
```

Normalize spaces:

```bash
tr -s '[:space:]' ' ' < file.txt
```

Extract digits:

```bash
echo "ID: 45X9" | tr -cd '0-9'
```

* * *

## 15\. Tips and Best Practices

- Use `tr` for simple transformations
    
- Prefer character classes for portability
    
- Combine with pipes for efficiency
    
- Avoid `cat` when redirection is enough
    

* * *

## 16\. Summary Cheat Sheet

```bash
tr 'a-z' 'A-Z'
tr -d '0-9'
tr -s ' '
tr -cd 'a-zA-Z'
tr '[:lower:]' '[:upper:]'
```

* * *

## Final Notes

`tr` is best for fast, simple character transformations. It complements `grep` (search), `sed` (line editing), and `awk` (field processing) in the Linux text-processing toolkit.

* * *

* * *

# `cut` Command in Linux

`cut` is a simple and fast Linux command used to **extract sections from each line of input**. It works on **columns, characters, or bytes** and is commonly used with delimited text such as CSV files, logs, and command output.

* * *

## 1\. What is `cut`

`cut` extracts **specific portions of each line** from standard input or files.

Typical use cases:

- Extract columns from CSV or TSV files
    
- Get usernames from `/etc/passwd`
    
- Parse command output
    

* * *

## 2\. How `cut` Works

```
input → split by delimiter / position → selected fields → output
```

Key points:

- Processes input line by line
    
- Does not understand quotes or escaping
    
- Best for simple, well-structured text
    

* * *

## 3\. Basic Syntax

```bash
cut OPTION... [FILE...]
```

Examples:

```bash
cut -c 1-5 file.txt
cut -d ':' -f 1 /etc/passwd
```

* * *

## 4\. Selecting Fields (`-f`)

Use `-f` to select fields separated by a delimiter.

```bash
cut -d ':' -f 1 /etc/passwd
```

Multiple fields:

```bash
cut -d ':' -f 1,3,6 /etc/passwd
```

* * *

## 5\. Field Delimiters (`-d`)

Default delimiter is **TAB**.

Custom delimiter:

```bash
cut -d ',' -f 2 data.csv
```

Pipe character:

```bash
cut -d '|' -f 1 file.txt
```

* * *

## 6\. Selecting Characters (`-c`)

Extract characters by position:

```bash
cut -c 1-4 file.txt
```

Single character:

```bash
cut -c 3 file.txt
```

* * *

## 7\. Selecting Bytes (`-b`)

Extract bytes instead of characters:

```bash
cut -b 1-10 file.bin
```

Difference:

- `-c` works on characters
    
- `-b` works on raw bytes (important for multibyte encodings)
    

* * *

## 8\. Ranges and Lists

| Format | Meaning |
| --- | --- |
| `1-5` | Range |
| `1,3,5` | List |
| `-4` | From start to 4 |
| `6-` | From 6 to end |

Examples:

```bash
cut -c -5 file.txt
cut -f 2- data.csv
```

* * *

## 9\. Working with Files

Single file:

```bash
cut -d ':' -f 1 file.txt
```

Multiple files:

```bash
cut -d ',' -f 1 file1.csv file2.csv
```

Redirect output:

```bash
cut -f 1 data.txt > out.txt
```

* * *

## 10\. Using `cut` in Pipelines

With `cat`:

```bash
cat file.txt | cut -f 2
```

With `grep`:

```bash
grep error log.txt | cut -d ' ' -f 1,5
```

With `awk`:

```bash
awk '{ print $0 }' file.txt | cut -c 1-20
```

* * *

## 11\. Handling Missing Delimiters (`-s`)

Suppress lines without delimiters:

```bash
cut -d ':' -f 1 -s file.txt
```

Without `-s`, entire lines are printed.

* * *

## 12\. Output Delimiter (`--output-delimiter`)

Change output field separator:

```bash
cut -d ':' -f 1,3 --output-delimiter=',' /etc/passwd
```

* * *

## 13\. Common Use Cases

Extract usernames:

```bash
cut -d ':' -f 1 /etc/passwd
```

Get file permissions:

```bash
ls -l | cut -c 1-10
```

Extract first column from CSV:

```bash
cut -d ',' -f 1 data.csv
```

* * *

## 14\. Limitations of `cut`

- No regular expressions
    
- Cannot handle quoted delimiters (CSV edge cases)
    
- Fixed delimiter only
    
- No conditional logic
    

For complex parsing, use `awk`.

* * *

## 15\. Tips and Best Practices

- Use `cut` for simple column extraction
    
- Prefer `-f` with clear delimiters
    
- Avoid using `cut` for complex CSV files
    
- Combine with `grep` and `sort`
    

* * *

## 16\. Summary Cheat Sheet

```bash
cut -d ':' -f 1 file
cut -d ',' -f 2,3 file
cut -c 1-5 file
cut -b 1-10 file
cut -f 2- file
```

* * *

## Final Notes

`cut` is ideal for fast, simple extraction tasks. It pairs well with `grep`, `sed`, `awk`, and `tr` in Linux text-processing pipelines.

* * *

* * *

# `paste` Command in Linux

`paste` is a Linux command used to **merge lines of files horizontally**. Unlike `cat`, which joins files vertically, `paste` combines corresponding lines from multiple files into a single line using a delimiter.

* * *

## 1\. What is `paste`

`paste` merges lines from one or more files **column-wise**.

Typical use cases:

- Combine related data from multiple files
    
- Create simple tables
    
- Merge command outputs
    

* * *

## 2\. How `paste` Works

```
file1 line1 ─┐
file2 line1 ─┼─> merged line
file3 line1 ─┘

file1 line2 ─┐
file2 line2 ─┼─> merged line
file3 line2 ─┘
```

Key points:

- Reads one line from each file at a time
    
- Uses TAB as the default delimiter
    
- Stops when the longest file ends
    

* * *

## 3\. Basic Syntax

```bash
paste [OPTIONS] [FILE...]
```

Example:

```bash
paste file1.txt file2.txt
```

* * *

## 4\. Merging Files Side by Side

Given:

`file1.txt`

```
A
B
C
```

`file2.txt`

```
1
2
3
```

Command:

```bash
paste file1.txt file2.txt
```

Output:

```
A	1
B	2
C	3
```

* * *

## 5\. Delimiters (`-d`)

Change the delimiter from TAB to another character.

Comma:

```bash
paste -d ',' file1.txt file2.txt
```

Multiple delimiters (cycled):

```bash
paste -d ',:' file1.txt file2.txt file3.txt
```

* * *

## 6\. Serial Mode (`-s`)

Serial mode merges lines from **one file at a time**.

```bash
paste -s file.txt
```

With delimiter:

```bash
paste -s -d ',' file.txt
```

Example:

```
A,B,C
```

* * *

## 7\. Working with Standard Input

Use `-` to read from standard input.

```bash
echo -e "a\nb\nc" | paste -s -d ':'
```

Combine stdin and file:

```bash
paste - file2.txt
```

* * *

## 8\. Combining Files of Unequal Length

If files have different lengths, `paste` continues until the **longest file ends**.

Missing fields are left empty.

```bash
paste file1.txt file2.txt
```

* * *

## 9\. Using `paste` in Pipelines

With `cut`:

```bash
cut -d ':' -f 1 /etc/passwd | paste -s -d ','
```

With `awk`:

```bash
awk '{ print $1 }' file.txt | paste -s
```

With `tr`:

```bash
tr '\n' '\t' < file.txt
```

* * *

## 10\. Common Use Cases

Create a simple table:

```bash
paste names.txt scores.txt
```

Join headers with data:

```bash
paste header.txt data.txt
```

Convert rows to columns:

```bash
paste -s -d '\t' file.txt
```

* * *

## 11\. Limitations of `paste`

- No conditional logic
    
- No pattern matching
    
- No awareness of field structure
    
- Limited delimiter handling
    

For advanced processing, use `awk`.

* * *

## 12\. Tips and Best Practices

- Use `paste` for horizontal merging
    
- Prefer TAB or simple delimiters
    
- Combine with `cut` for column control
    
- Use `awk` for complex joins
    

* * *

## 13\. Summary Cheat Sheet

```bash
paste file1 file2
paste -d ',' file1 file2
paste -s file
paste -s -d ':' file
paste - file
```

* * *

## Final Notes

`paste` is a simple yet effective tool for combining files side by side. It complements `cut` (extraction), `tr` (character translation), `sed` (line editing), and `awk` (field processing) in Linux text-processing workflows.

* * *

* * *

# `split` Command in Linux

`split` is a Linux command used to **divide large files into smaller pieces**. It is commonly used for handling big logs, transferring large files, or processing data in chunks.

* * *

## 1\. What is `split`

`split` breaks a file into multiple smaller files of equal size (by lines or bytes).

Typical use cases:

- Splitting large log files
    
- Preparing files for upload or transfer limits
    
- Parallel processing of data
    

* * *

## 2\. How `split` Works

```
large_file → chunk1, chunk2, chunk3, ...
```

Key points:

- Output files are created sequentially
    
- Default output prefix is `x`
    
- Default split is by number of lines
    

* * *

## 3\. Basic Syntax

```bash
split [OPTIONS] [FILE] [PREFIX]
```

Example:

```bash
split file.txt
```

This creates files like `xaa`, `xab`, `xac`.

* * *

## 4\. Splitting by Number of Lines

Split every 1000 lines (default behavior):

```bash
split -l 1000 file.txt
```

Custom number of lines:

```bash
split -l 50 file.txt part_
```

* * *

## 5\. Splitting by File Size

Split by bytes:

```bash
split -b 1M file.txt
```

Supported suffixes:

- `K` = kilobytes
    
- `M` = megabytes
    
- `G` = gigabytes
    

Exact bytes:

```bash
split -b 1024 file.txt
```

* * *

## 6\. Numeric and Alphabetic Suffixes

Default suffixes are alphabetic:

```
xaa, xab, xac
```

Use numeric suffixes:

```bash
split -d file.txt part_
```

Specify suffix length:

```bash
split -d -a 3 file.txt part_
```

Output:

```
part_000, part_001, part_002
```

* * *

## 7\. Custom Prefixes

Specify output file prefix:

```bash
split file.txt chunk_
```

Result:

```
chunk_aa, chunk_ab, chunk_ac
```

* * *

## 8\. Splitting Standard Input

Split data from a pipeline:

```bash
cat file.txt | split -l 100 - out_
```

Use `-` to indicate standard input.

* * *

## 9\. Recombining Split Files

Recombine using `cat`:

```bash
cat part_* > original.txt
```

Order matters. Ensure correct filename sorting.

* * *

## 10\. Using `split` in Pipelines

With `grep`:

```bash
grep error large.log | split -l 500 - error_
```

With `awk`:

```bash
awk '{ print }' data.txt | split -b 1M - data_
```

* * *

## 11\. Common Use Cases

Split large log file:

```bash
split -l 10000 access.log log_
```

Split CSV by size:

```bash
split -b 5M data.csv csv_
```

Prepare email attachments:

```bash
split -b 10M archive.tar.gz part_
```

* * *

## 12\. Limitations of `split`

- No content-aware splitting
    
- No header preservation
    
- Cannot split by patterns
    

For advanced splitting, use `awk`.

* * *

## 13\. Tips and Best Practices

- Choose meaningful prefixes
    
- Use numeric suffixes for large splits
    
- Verify order before recombining
    
- Combine with `gzip` for compression
    

* * *

## 14\. Summary Cheat Sheet

```bash
split file
split -l 100 file
split -b 1M file
split -d -a 3 file part_
split -l 50 - out_
cat part_* > file
```

* * *

## Final Notes

`split` is ideal for managing large files efficiently. It works well alongside `cat`, `grep`, `awk`, and `gzip` in Linux data-processing workflows.

* * *

* * *

# `join` Command in Linux

`join` is a Linux command used to **combine lines from two files based on a common field**. It is similar to a database **JOIN operation** and is especially useful for working with structured, sorted text files.

* * *

## 1\. What is `join`

`join` merges two files line by line using a **common join key**.

Typical use cases:

- Joining user IDs with user details
    
- Merging CSV or TSV datasets
    
- Performing relational-style joins on text files
    

* * *

## 2\. How `join` Works

```
file1 (key, data...) + file2 (key, data...) → merged output
```

Key points:

- Works on **two files only**
    
- Both files must be **sorted on the join field**
    
- Default join field is the **first field**
    

* * *

## 3\. Basic Syntax

```bash
join [OPTIONS] file1 file2
```

Example:

```bash
join file1.txt file2.txt
```

* * *

## 4\. Input File Requirements

- Files must be sorted
    
- Join field must match exactly
    
- Default delimiter is **space or TAB**
    

Sort before joining:

```bash
sort file1.txt -o file1.txt
sort file2.txt -o file2.txt
```

* * *

## 5\. Joining on Default Fields

Given:

`file1.txt`

```
1 Alice
2 Bob
3 Carol
```

`file2.txt`

```
1 HR
2 IT
3 Sales
```

Command:

```bash
join file1.txt file2.txt
```

Output:

```
1 Alice HR
2 Bob IT
3 Carol Sales
```

* * *

## 6\. Specifying Join Fields (`-1`, `-2`)

Join using different fields:

```bash
join -1 2 -2 1 file1.txt file2.txt
```

- `-1 N`: join field in file1
    
- `-2 N`: join field in file2
    

* * *

## 7\. Field Delimiters (`-t`)

Use a custom delimiter:

```bash
join -t ',' file1.csv file2.csv
```

Tab delimiter:

```bash
join -t $'\t' file1.tsv file2.tsv
```

* * *

## 8\. Output Formatting (`-o`)

Control which fields are printed:

```bash
join -o 1.1,1.2,2.2 file1.txt file2.txt
```

Meaning:

- `1.1` → file1 field 1
    
- `2.2` → file2 field 2
    

* * *

## 9\. Including Unmatched Lines (`-a`)

Include unpaired lines from file1:

```bash
join -a 1 file1.txt file2.txt
```

Include from both files:

```bash
join -a 1 -a 2 file1.txt file2.txt
```

* * *

## 10\. Printing Unpaired Lines (`-v`)

Print only unmatched lines:

```bash
join -v 1 file1.txt file2.txt
```

From file2:

```bash
join -v 2 file1.txt file2.txt
```

* * *

## 11\. Replacing Missing Fields (`-e`)

Replace missing values with placeholder:

```bash
join -a 1 -e 'NA' file1.txt file2.txt
```

* * *

## 12\. Working with Unsorted Files

Use process substitution:

```bash
join <(sort file1.txt) <(sort file2.txt)
```

* * *

## 13\. Using `join` in Pipelines

With `cut`:

```bash
cut -d ',' -f 1,2 a.csv | join -t ',' - b.csv
```

With `awk`:

```bash
awk '{ print $1, $2 }' file1 | join - file2
```

* * *

## 14\. Common Use Cases

Join passwd and group info:

```bash
join -t ':' <(sort /etc/passwd) <(sort /etc/group)
```

Merge ID and score files:

```bash
join ids.txt scores.txt
```

* * *

## 15\. Limitations of `join`

- Only two files at a time
    
- Requires sorted input
    
- No complex conditions
    
- Limited CSV support
    

For advanced joins, use `awk` or databases.

* * *

## 16\. Tips and Best Practices

- Always sort before joining
    
- Use `-t` explicitly for CSV/TSV
    
- Use `-o` to control output
    
- Prefer `awk` for complex joins
    

* * *

## 17\. Summary Cheat Sheet

```bash
join f1 f2
join -t ',' f1 f2
join -1 2 -2 1 f1 f2
join -a 1 -a 2 f1 f2
join -v 1 f1 f2
join -o 1.1,2.2 f1 f2
```

* * *

## Final Notes

`join` brings relational-style data merging to the command line. It works best with clean, sorted, structured files and pairs well with `sort`, `cut`, `paste`, and `awk`.

* * *

* * *
